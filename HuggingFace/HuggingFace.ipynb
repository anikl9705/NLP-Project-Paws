{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HuggingFace.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1xuaUdzVlVdxUOUeB1NpsuKgbi4UxEXI0","authorship_tag":"ABX9TyNiQO4pEF0IouE5npg6zNaF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"g7a_-c5fvbWa","colab_type":"code","outputId":"74f453fa-9f9d-42fa-8dd5-c0e709eb472c","executionInfo":{"status":"ok","timestamp":1584910702690,"user_tz":420,"elapsed":6817,"user":{"displayName":"Anirudh Krishna Lakshmanan","photoUrl":"","userId":"17063214427478499728"}},"colab":{"base_uri":"https://localhost:8080/","height":391}},"source":["!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.23)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.23 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.23)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers) (2.8.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers) (0.15.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q3MLqgPywahc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8ebcc535-4696-4145-bf06-9f1d17dad2ba","executionInfo":{"status":"ok","timestamp":1584910702692,"user_tz":420,"elapsed":6810,"user":{"displayName":"Anirudh Krishna Lakshmanan","photoUrl":"","userId":"17063214427478499728"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nxHwnOz-wElJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":63},"outputId":"e6924492-5b66-429e-f5d8-8f1607b4120d","executionInfo":{"status":"ok","timestamp":1584910703574,"user_tz":420,"elapsed":7683,"user":{"displayName":"Anirudh Krishna Lakshmanan","photoUrl":"","userId":"17063214427478499728"}}},"source":["import torch\n","from transformers import *\n","import numpy as np\n","import scipy as scipy\n","import pandas as pd\n","import os\n","import ast\n","import tqdm as tqdm\n","\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"DdJNWG3X01oV","colab_type":"code","colab":{}},"source":["MODEL_CLASS, TOKENIZER_CLASS, PRETRAINED = (DistilBertForSequenceClassification, DistilBertTokenizer, 'distilbert-base-cased')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7BPg3_DtVIYJ","colab_type":"text"},"source":["# **Preprocess Data** "]},{"cell_type":"code","metadata":{"id":"Q_9WZx3Jwwjd","colab_type":"code","colab":{}},"source":["data_path = '/content/drive/My Drive/Colab/data_qqp/'\n","\n","train_data = pd.read_table(os.path.expanduser(data_path+'train.tsv'),header=0)\n","test_data = pd.read_table(os.path.expanduser(data_path+'dev_and_test.tsv'),header=0)\n","\n","train_data['sentence1'] = train_data['sentence1'].apply(lambda x: ast.literal_eval(x).decode())\n","train_data['sentence2'] = train_data['sentence2'].apply(lambda x: ast.literal_eval(x).decode())\n","\n","test_data['sentence1'] = test_data['sentence1'].apply(lambda x: ast.literal_eval(x).decode())\n","test_data['sentence2'] = test_data['sentence2'].apply(lambda x: ast.literal_eval(x).decode())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DXB1JZKf0o37","colab_type":"code","colab":{}},"source":["display(test_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dI0CzB3p0uT5","colab_type":"code","colab":{}},"source":["class Tokenizer:\n","    # init\n","    def __init__(self, tokenizer_class, pretrained_weights, group_sent=False):\n","        self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n","        self.group_sent = group_sent\n","        \n","    # tokenize\n","    def tokenize_data(self, data):\n","        tokenized = pd.DataFrame(columns = ['i1', 's1', 'i2', 's2', 'i', 's', 'y'])\n","        for index, row in data.iterrows():\n","            token_row = {\n","                'i1' : [],\n","                's1' : [],\n","                'i2' : [],\n","                's2' : [],\n","                'i' : [],\n","                's' : [],\n","                'y': 0\n","            }\n","            sent1 = row['sentence1']\n","            encoding1 = self.tokenizer.encode_plus(sent1, return_token_type_ids=True, max_length=128, pad_to_max_length=True)\n","\n","            token_row['i1'] = encoding1['input_ids']\n","            token_row['s1'] = encoding1['token_type_ids']\n","\n","            # indexed1 = self.tokenizer.encode(sent1, max_length=128, pad_to_max_length=True)\n","            # print(token)\n","            # indexed1 = self.tokenizer.convert_tokens_to_ids(token)\n","            # indexed_tensor = torch.tensor([indexed1])\n","            # segment = np.zeros(len(indexed1), dtype=int)\n","            # segment_tensor = torch.tensor([segment])\n","            # token_row['s1'] = segment_tensor\n","\n","            sent2 = row['sentence2']\n","            encoding2 = self.tokenizer.encode_plus(sent2, return_token_type_ids=True, max_length=128, pad_to_max_length=True)\n","\n","            token_row['i2'] = encoding2['input_ids']\n","            token_row['s2'] = encoding2['token_type_ids']\n","            # indexed2 = self.tokenizer.encode(sent2, max_length=128, pad_to_max_length=True)\n","            # # indexed2 = self.tokenizer.convert_tokens_to_ids(token)\n","            # indexed_tensor = torch.tensor([indexed2])\n","            # token_row['i2'] = indexed_tensor\n","            # segment = np.zeros(len(indexed2), dtype=int)\n","            # segment_tensor = torch.tensor([segment])\n","            # token_row['s2'] = segment_tensor\n","            \n","            if self.group_sent == True:\n","                encoding = self.tokenizer.encode_plus(sent1, sent2, return_token_type_ids=True, max_length=256, pad_to_max_length=True)\n","\n","                token_row['i'] = encoding2['input_ids']\n","                token_row['s'] = encoding2['token_type_ids']\n","\n","                # print(encoding)\n","                # indexed = self.tokenizer.build_inputs_with_special_tokens(indexed1, indexed2)\n","                # token_row['i'] = torch.tensor([indexed])\n","                # # print(torch.tensor([indexed]).shape)\n","\n","                # max_length = max(len(indexed), max_length)\n","\n","                # if len(indexed) > 500:\n","                #     print(sent1 + sent2)\n","                # segment = self.tokenizer.create_token_type_ids_from_sequences(indexed1, indexed2)\n","                # token_row['s'] = torch.tensor([segment])\n","            \n","            token_row['y'] = row['label']\n","            tokenized = tokenized.append(token_row, ignore_index=True)\n","        return tokenized"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_0fYf8Of03Bd","colab_type":"code","colab":{}},"source":["tokenizer = Tokenizer(TOKENIZER_CLASS, PRETRAINED, True)\n","test_tokens = tokenizer.tokenize_data(test_data)\n","train_tokens = tokenizer.tokenize_data(train_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xbQj_-rF9TKm","colab_type":"code","colab":{}},"source":["display(test_tokens)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oZUOn4atS6ot","colab_type":"code","colab":{}},"source":["test_tokens.to_csv('/content/drive/My Drive/Colab/data_qqp/test_tok.csv')\n","train_tokens.to_csv('/content/drive/My Drive/Colab/data_qqp/train_tok.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zpNONuQgVot2","colab_type":"text"},"source":["# **Load Data**\n","- TBD: Create DataLoader"]},{"cell_type":"code","metadata":{"id":"bt7C-VZeVoH1","colab_type":"code","colab":{}},"source":["test_tokens = pd.read_csv('/content/drive/My Drive/Colab/data_qqp/test_tok.csv')\n","test_tokens['i'] = test_tokens['i'].apply(lambda x: ast.literal_eval(x))\n","test_tokens['s'] = test_tokens['s'].apply(lambda x: ast.literal_eval(x))\n","\n","train_tokens = pd.read_csv('/content/drive/My Drive/Colab/data_qqp/train_tok.csv')\n","train_tokens['i'] = train_tokens['i'].apply(lambda x: ast.literal_eval(x))\n","train_tokens['s'] = train_tokens['s'].apply(lambda x: ast.literal_eval(x))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8oujXdPUWwhk","colab_type":"code","colab":{}},"source":["# display(test_tokens['i'].values)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aa1IXRK-VFgu","colab_type":"text"},"source":["# **BERT Model**"]},{"cell_type":"code","metadata":{"id":"6GlyEXp_0yE1","colab_type":"code","colab":{}},"source":["class Model_Classifier:\n","    # initalize model\n","    def __init__(self, model_class, pretrained_weights):\n","        self.model = model_class.from_pretrained(pretrained_weights).cuda()\n","        self.TRAINING_ITERATIONS = 10000\n","        self.WARMUP = 500\n","        self.BATCH_SIZE = 64\n","        self.REPORT_FREQUENCY = 100\n","#         self.model.eval()\n","        \n","    # train classifier\n","    def train(self, data):\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","            {\"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n","        ]\n","        optimizer = AdamW(optimizer_grouped_parameters)\n","        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.WARMUP, num_training_steps=self.TRAINING_ITERATIONS)\n","        self.model.train()\n","        bar = tqdm.tqdm(total=1)\n","        for i in range(0, self.TRAINING_ITERATIONS):\n","            train_data = data.sample(n=self.BATCH_SIZE)\n","            train_ids = torch.tensor(train_data['i'].values.tolist()).cuda()\n","            train_segments = torch.tensor(train_data['s'].values.tolist()).cuda()\n","            train_labels = torch.tensor(train_data['y'].values.tolist()).cuda()\n","            \n","            # Uncomment this for BERT\n","            # outputs = self.model(input_ids = train_ids, token_type_ids = train_segments, labels = train_labels)\n","\n","            # Comment this for DistilBERT\n","            outputs = self.model(input_ids = train_ids, labels = train_labels)\n","            loss = outputs[0].cuda()\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","            self.model.zero_grad()\n","            bar.update(1)\n","            if i % self.REPORT_FREQUENCY == 0:\n","                bar.close()\n","                print(\"%d: Loss - %s\" %(i, loss))\n","                bar = tqdm.tqdm(total=100)\n","        bar.close()\n","        return\n","\n","    # save model\n","    def save(self):\n","      self.model.save_pretrained('/content/drive/My Drive/Colab/model/')\n","    \n","    # predict values\n","    def test(self, data):\n","        self.model.eval()\n","        ids = data['i'].values\n","        segments = data['s'].values\n","        labels = data['y'].values\n","        for i in range(len(ids)):\n","            outputs = self.model(input_ids = ids[i], token_type_ids = segments[i])\n","            probs = outputs[0][0]\n","            print(\"pred: %f, act: %f\" %(probs.max(0)[1], labels[i]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WKgl5Ks904oA","colab_type":"code","colab":{}},"source":["bert_class = Model_Classifier(MODEL_CLASS, PRETRAINED)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qQGE_PiiaSOK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":782},"outputId":"6880c1c8-3461-4f57-bbe0-e3f0b8dde01a"},"source":["bert_class.train(train_tokens)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["0: Loss - tensor(0.6752, device='cuda:0', grad_fn=<NllLossBackward>)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["100: Loss - tensor(0.4605, device='cuda:0', grad_fn=<NllLossBackward>)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["200: Loss - tensor(0.6807, device='cuda:0', grad_fn=<NllLossBackward>)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["300: Loss - tensor(0.6702, device='cuda:0', grad_fn=<NllLossBackward>)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["400: Loss - tensor(0.7297, device='cuda:0', grad_fn=<NllLossBackward>)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["500: Loss - tensor(0.5044, device='cuda:0', grad_fn=<NllLossBackward>)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["600: Loss - tensor(0.6039, device='cuda:0', grad_fn=<NllLossBackward>)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["700: Loss - tensor(0.5879, device='cuda:0', grad_fn=<NllLossBackward>)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100/100 [01:09<00:00,  1.45it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["800: Loss - tensor(0.5958, device='cuda:0', grad_fn=<NllLossBackward>)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["900: Loss - tensor(0.6439, device='cuda:0', grad_fn=<NllLossBackward>)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["1000: Loss - tensor(0.5613, device='cuda:0', grad_fn=<NllLossBackward>)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["1100: Loss - tensor(0.6222, device='cuda:0', grad_fn=<NllLossBackward>)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["1200: Loss - tensor(0.6780, device='cuda:0', grad_fn=<NllLossBackward>)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["1300: Loss - tensor(0.6075, device='cuda:0', grad_fn=<NllLossBackward>)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["1400: Loss - tensor(0.5813, device='cuda:0', grad_fn=<NllLossBackward>)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["1500: Loss - tensor(0.6452, device='cuda:0', grad_fn=<NllLossBackward>)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["1600: Loss - tensor(0.6821, device='cuda:0', grad_fn=<NllLossBackward>)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["1700: Loss - tensor(0.7098, device='cuda:0', grad_fn=<NllLossBackward>)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["1800: Loss - tensor(0.6322, device='cuda:0', grad_fn=<NllLossBackward>)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["1900: Loss - tensor(0.5992, device='cuda:0', grad_fn=<NllLossBackward>)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["2000: Loss - tensor(0.6517, device='cuda:0', grad_fn=<NllLossBackward>)\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n","  0%|          | 0/100 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["2100: Loss - tensor(0.5537, device='cuda:0', grad_fn=<NllLossBackward>)\n"],"name":"stdout"},{"output_type":"stream","text":[" 47%|████▋     | 47/100 [00:32<00:36,  1.45it/s]"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"ru_vFuEXZUKh","colab_type":"code","colab":{}},"source":["bert_class.test(test_tokens)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QdbvnMYHVhho","colab_type":"text"},"source":["# **Custom Model**"]},{"cell_type":"code","metadata":{"id":"h355NpHZ0zv0","colab_type":"code","colab":{}},"source":["class Model:\n","    # initalize model\n","    def __init__(self, model_class, tokenizer_class, pretrained_weights):\n","        self.model = model_class.from_pretrained(pretrained_weights)\n","        \n","    # Generate embeddings\n","    def get_embeddings(self, data):\n","        return\n","        \n","    # \n","    def train(self, data):\n","        return\n","    \n","    # \n","    def test(self, data):\n","        return"],"execution_count":0,"outputs":[]}]}